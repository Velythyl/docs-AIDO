# General rules {#other-rules status=ready}  

<minitoc />

## Protocol

### Deployment technique

We use Docker containers to package, deploy, and run the applications on the physical Duckietown platform as well as on the cloud for simulation. Base Docker container images are provided and distributed via [Docker HUB][dockerhub].

[dockerhub]: https://hub.docker.com/r/duckietown/

A **challenges server** is used to collect and queue all submitted agents. The **simulation evaluations** execute each queued agents as they become available. Submissions that pass the simulation environment will be queued for execution in the Autolab.

<!-- <div figure-id="fig:dockerflow">
\input{dockerflow.tex}
<figcaption>Submission, Deployment, and Execution Flow
</figcaption>
</div> -->


For validation of submitted code and evaluation the competition finals a surprise environment will be employed. This is to discourage overfitting to any particular Duckietown configuration.


### Submission of entries

Participants can submit their code in the form of a docker container to a task. Scripts are provided for creating the container image in a conforming way.

The system will schedule to run the code on the cloud on the challenges selected by the user, and, if simulations pass, on the Autolabs.

Participants can submit entries as many times as they would like. Access control policies are to be implemented, should certain participants monopolize the computational and physical resources available.

Participants are required to open source their solutions source code. If auxiliary training data are used to train the models, that data must be made available.

Submitted code will be evaluated in simulation and if sufficient on physical Autolabs. Scores and logs generated with submitted code will be made available.


### Simulators {status=beta}

[Simulation code](https://github.com/duckietown/gym-duckietown/) is available as open source for everybody to use on computers that they control. The simulatored baselines interact with the simulator through a standardized interfaces that mimics the interface with the real robot. 

### Autolab test and validation

<!-- If there are $n$ robotariums available, $n-1$ robotariums can be used for training and testing, while 1 robotarium is used for validation. -->

When an experiment is run in a **training/testing** Autolab, the participants receive, in addition to the score, detailed feedback, including logs, telemetry, videos, etc. The sensory data generated by the robots is continuously recorded and becomes available immediately to the entire community.

When an experiment is run in a **validation** Autolab, the only output to the user is the test score and minimal statistics (number of collisions, number of rule violations, etc.)

### Leaderboards

After each run in an Autolab, the participants can see the metrics statistics on the [competition scoring website](https://challenges.duckietown.org).


## Eligibility

Employees and affiliates of Motional/SwissRe are ineligible from participation in the competition. Employees and affiliates of Motional/SwissRe may submit baseline solutions that will be reported in a special leaderboard.

Students of ETH ZÃ¼rich, Montreal, and TTIC, are eligible to participate in the competition as part of coursework, if they do not work in the organization of the competition.

## Intellectual property

Participants of AI-DO are required to provide the source code / data / learning models of their submission to the organizers before the finals (so that we can check for their regularity.)

Winners of AI-DO are required to make their submission open source so that
it can be reused later in the next challenges.