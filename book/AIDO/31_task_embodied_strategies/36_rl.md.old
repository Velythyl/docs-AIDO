# Reinforcement Learning Workflow {#embodied_rl status=draft}

## Setting up your simulator {#simulator_setup}


This is a tutorial on how to make a submission for the **LF**
(lane following on a closed course), **LFV** (lane following
with dynamic obstacles), and the **NAVV** (Navigation with dynamic vehicles) AI-DO challenges. These challenges all use
the `gym-duckietown` environment from https://github.com/duckietown/gym-duckietown
but pack it up into a docker container.

To this end the `gym-duckietown` environment lives in one
container called `gym-duckietown-server`. The code for the agent that
**you** as the participant have to modify as well as the code for
communicating with the `-server` container live in a different
repository called `gym-duckietown-agent`. We will guide you
through the process of installing everything, forking this repo,
modifying the code and making a submission.

**TL;DR**
There are two containers:

- `gym-duckietown-server`, runs the actual gym environment
- `gym-duckietown-agent`, hold the agent code (i.e. your code)

We provide scripts for pulling, building and launching both containers.

### Prerequisites

Currently we support development on Mac and Linux. We are working on supporting Windows soon.

Please make sure you have the following installed on your PC:

- [Docker CE](https://docs.docker.com/install/) for running the containers
- [Docker Compose](https://docs.docker.com/compose/install/#install-compose) for starting both `server` and `agent` and make them communicate with each other in one command
- Some Python code editor for modifying the agent. We'd recommend [PyCharm](https://www.jetbrains.com/pycharm/) or [Atom](https://atom.io/)
- [Git](https://git-scm.com/downloads) for pulling the repositoy.

### Install

Point your browser to this URL: https://github.com/duckietown/gym-duckietown-agent, click the `Fork` button (top right) and then click your Github account when it asks you where you want to fork to.

<figure id='screen-gym-fork'>
<figcaption></figcaption>
<img src="screenshots/screenshot-gym-duckietown-agent-fork2.png" class='diagram' style="width:100%"/>
</figure>

Now you should see the page of your forked repo. Now let's clone that new repo. Click the green `Clone or Download` button (right side), copy the URL, open a terminal and clone your repo

    git clone https://github.com/[YOUR GITHUB USERNAME HERE]/gym-duckietown-agent.git

That's it for the "installation" part. :)

### Run

Change into the directoy of you `gym-duckietown-agent`

    cd gym-duckietown-agent

and launch both containers (on the LF task) with this command

    docker-compose -f docker-compose-lf.yml pull && \
    docker-compose -f docker-compose-lf.yml up

What this will do:

- Pull the latest `gym-duckietown-server` container from dockerhub (this is as of writing `1.9GB` and can take a bit to download) and run it
- Build the agent into a container (including installing dependencies within the container) and run it

Both of these actions should only take a bit longer on the first start. On consecutive starts this should be faster.

In the process of starting both containers you should see this:

<figure id='screen-gym-start'>
<figcaption></figcaption>
<img src="screenshots/screenshot-gym-duckietown-agent-startup.png" class='diagram' style="width:100%"/>
</figure>

Then the two containers should run for a few seconds, generating steps and simulating the results. After that's done, you should see the final verdict, the average reward over 10 episodes in the terminal:

<figure id='screen-gym-end'>
<figcaption></figcaption>
<img src="screenshots/screenshot-gym-duckietown-agent-end.png" class='diagram' style="width:100%"/>
</figure>

This second to last line (the one with `The average reward of 10 episodes was -538.5323. Best epi....`) - that's your self-evaluation. That's how you measure your own performance. You should try to get this number as high as possible, but also keep in mind that there is always some randomness involved. So even if you run this twice without changing anything, the number can change quite a lot. Later in development you can modify how many episodes are averaged so that you get a better estimate. Currently that's 10 but that's quite low.

In order to stop the server and get your terminal back, press <kbd>CTRL</kbd>+<kbd>c</kbd>

### Seeing the Policy in Action aka Visual Debugging

If you want to see what the Duckiebot move around, the easiest way to do so is by running the `agent` code without a container (i.e. on your local machine). In order to do that, first install the `gym-duckietown-agent` package locally (in a terminal):

You need `python3` installed, as well as `pip3` and we assume you have sudo access the machine.

    # assuming your current shell is
    # in the directoy "gym-duckietown-agent"
    sudo pip3 install -e .

If you don't have sudo access, you can run the code without the `sudo` and with `--user` instead, i.e. `pip3 install --user -e .`

Then launch the `gym-duckietown-server` container in the background (and change the task to either `LF` or `LFV` by changing the `DUCKIETOWN_CHALLENGE` parameter below):

    docker run -tid -p 8902:8902 -p 5558:5558 \
    -e DISPLAY=$DISPLAY \
    -e DUCKIETOWN_CHALLENGE=LF \
    --name gym-duckietown-server \
    --rm -v /tmp/.X11-unix:/tmp/.X11-unix \
    duckietown/gym-duckietown-server

(If you ever want to stop this `server` container, you can use the command `docker container stop gym-duckietown-server` but **DON'T** do this now.)

And now you can the agent on the shell:

    python3 agent.py

And after a second or so, the matplotlib window should pop up and you should see your Duckiebot in action.

<figure id='screen-gym-render'>
<figcaption></figcaption>
<img src="screenshots/matplotlib-render-animated.gif" class='diagram' style="width:100%"/>
</figure>

---

The Duckietown team wishes you happy coding and best of luck! :)

---

### FAQ

#### 1. "It looks like it's stuck / I see output like `sent action: 0 ...` but I don't see the line where it says `The average reward...`. What do?"

Try removing any old `gym-duckietown-server` containers. You can do this by trying out the following commands:


    docker container stop \
    gym-duckietown-agent_gym-duckietown-server_1; \
    docker container rm \
    gym-duckietown-agent_gym-duckietown-server_1

as well as

    docker container stop gym-duckietown-server; \
    docker container rm gym-duckietown-server


#### 2. "I see a lot of lines that read something like `gym-duckietown-server_1  | step_count = 29, reward=-2.018, done = False` but I don't see the line where it says `The average reward...` . What do?"

That's an easy one - the output of your agent is burried under all the console output of the server. That happens sometimes, but not super consistently. To alleviate this, just add `| less` after your `docker-compose up` command like so:


    docker-compose pull && docker-compose up | less

This way all the ouput is piped into the `less` text viewer program and you can scroll through it at your leisure (with the arrow keys). You can close the `less` program by pressing the <kbd>q</kbd> key.


#### 3. "I get the error `Bind for 0.0.0.0:8902 failed: port is already allocated`"

The full error looks something like this:

    Creating gym-duckietown-agent_gym-duckietown-server_1 ... error

    ERROR: for gym-duckietown-agent_gym-duckietown-server_1  Cannot start service gym-duckietown-server: driver failed programming external connectivity on endpoint gym-duckietown-agent_gym-duckietown-server_1 (0dddcea67fab4de58153d323788cc1baced4415c90471fce04fde2c1180273ea): Bind for 0.0.0.0:8902 failed: port is already allocated

    ERROR: for gym-duckietown-server  Cannot start service gym-duckietown-server: driver failed programming external connectivity on endpoint gym-duckietown-agent_gym-duckietown-server_1 (0dddcea67fab4de58153d323788cc1baced4415c90471fce04fde2c1180273ea): Bind for 0.0.0.0:8902 failed: port is already allocated
    ERROR: Encountered errors while bringing up the project.

The problem here is that there is some container or service running that is occupying the same port (probably that's another `gym-duckietown-server` container that hasn't shut down properly). You can fix this by following the same steps as in answer 1.


## Reinforcement Learning

Reinforcement learning in general means that your learning agent receives inputs (**"observations"**) at every timestep, and has to provide an output (**"action"**) in return, but you don't know what the correct action is. In order to guide the action, however, you receive a feedback (**"reward"**) on how good or bad kind of a state you are in now (after this last action). Just to make that entirely clear: *the reward you receive is not necessarily for the last action you took. It's the reward for the state you find yourself in now and that includes all the actions you took so far*. Therefore a mistake you made some time ago can show a lot later, or equivalently a correct move could take a few timesteps to show as higher reward.

The tasks `LF`,`LFV`, and `NAVV` follow the OpenAI Gym convention of providing reinforcement learning environments. That means each of these tasks can be used directly with existing RL algorithm implementation. Here is how it works.

The general pattern of any gym environment is this:

```python
import gym
import NAME_OF_GYM_EXTENSION

env = gym.make("NAME-OF-ENVIRONMENT")

observation = env.reset()

EPISODES = 10

done = False

for episode in range(EPISODES):
  while not done:

    action = PICK_ACTION(observation)

    observation, reward, done, misc = env.step(action)

    UPDATE_AGENT(observation, reward, done, misc)

    # this next line is optional
    env.render("human")

  env.reset()

env.close()
```

The ingredients you need to make this work are these:

- `NAME_OF_GYM_EXTENSION` - Either you are using a gym environment that ships with OpenAI gym (then you don't need this line), or you are using a custom environment like the one we are providing for this task and then you have to import the custom environment in addition to `gym`. This is `gym_duckietown_agent` in our specific case.
- `NAME-OF-ENVIRONMENT` - Is the name of the environment of the concrete reinforcement learning problem you are trying to solve, so for example in our case for every task there is one such environment. The names are `Duckietown-Lf-v0`, `Duckietown-Lfv-v0`, and `Duckietown-Navv-v0` for the tasks `LF`, `LFV`, and `NAVV` respectively.
- `EPISODES` - Is a number (integer `≥1` ) of your choosing. During training this will determine how long your algorithm is training. During testing this will determine how well your reward is averaged. The latter means that between individual episodes you can get wildly different results due to randomness. Therefore you should evaluate with a high number for more consistent results. Each episode will automatically run only for a given number of timesteps. This is determined by each task and you can look it up in the source code if you want.
- `PICK_ACTION` - This is your agent code for selecting an action, i.e. the inference part. You have to implement this by yourself. The selection of an action is usually dependent on the last known observation, but feel free to do this however you like. You can also for example completely ignore the last observation and just randomly sample a possible action from the action space like so: `action = env.action_space.sample()`. That's a function that all gym environments include.
- `UPDATE_AGENT` - This is also your agent code, but this time it's the function that adds new knowledge to your agent. When you're implementing this function, you usually pay attention to the observation you got with respect to the previous observation and your action (i.e. to learn how your action affects the environment), to the reward you got w.r.t to the action (i.e. to learn how your action affects rewards), or to the misc variable to see if the environment has any additional data for you, or any combination of the above. `done` is usually also included in learning agents to see which actions lead to a termination of the episode. This can be either catastrophic failure like driving into obstacles or this can be "good" and just your agent reaching the maximum possible steps for an episode.

As for what the standard gym functions are doing:

- `env.reset()` - Is used (a) to initialize the environment and (b) to reset the environment in case of failure or max steps. This function always returns an observation.
- `env.step(action)` - Takes an action and executes it in the environment. This returns a 4-tuple of next observation, reward, done-ness, and misc. Done-ness means is the current episode over? If this is `True`, then the environment needs to be `reset()`.
- `env.render(mode)` - This is entirely optional. Many environments support rendering the state of the environment so that researchers can observe their agents (either during training or testing). In order to get the human observer mode, run this with `mode = "human"`. Another common example is `env.render("rgb_array")` which returns the same observation, but not as a graphic or window but as a NumPy array. In our case the observation in the `LF`, `LFV`, and `NAVV` tasks is the camera feed and the `env.render()` shows you the camera feed with some additional data on top that the agent doesn't see (like the center of the road, orientation, etc.).
- `env.close()` - Is mostly here to adhere to the convention. There are some environments that need to be explicitly closed. Our doesn't. But it doesn't hurt to keep a good coding style and conform to this simple convention.

<!--If you want to see know about reinforcement learning or see an RL algorithm in action, please check out our  [Reinforcement Learning page](#reinforcement-learning).-->

---


## How to build your mdoel



## How to define the reward function



## How to turn your model into an AI-DO submission
